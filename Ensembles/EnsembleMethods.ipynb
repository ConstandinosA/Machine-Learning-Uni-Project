{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O8gU7AYPXMmA"
      },
      "source": [
        "## About iPython Notebooks ##\n",
        "\n",
        "iPython Notebooks are interactive coding environments embedded in a webpage. You will be using iPython notebooks in this class. Make sure you fill in any place that says `# BEGIN CODE HERE #END CODE HERE`. After writing your code, you can run the cell by either pressing \"SHIFT\"+\"ENTER\" or by clicking on \"Run\" (denoted by a play symbol). Before you turn this problem in, make sure everything runs as expected. First, **restart the kernel** (in the menubar, select Kernel$\\rightarrow$Restart) and then **run all cells** (in the menubar, select Cell$\\rightarrow$Run All). \n",
        "\n",
        " **What you need to remember:**\n",
        "\n",
        "- Run your cells using SHIFT+ENTER (or \"Run cell\")\n",
        "- Write code in the designated areas using Python 3 only\n",
        "- Do not modify the code outside of the designated areas\n",
        "- In some cases you will also need to explain the results. There will also be designated areas for that. \n",
        "\n",
        "Fill in your **NAME** and **AEM** below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "lO-jJrtNXMmH"
      },
      "outputs": [],
      "source": [
        "NAME = \"Κωνσταντίνος Ανδρέου\"\n",
        "AEM = \"9521\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sh0EE7BJXMmJ"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v_VpnGyWXMmK"
      },
      "source": [
        "# Assignment 3 - Ensemble Methods #"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2dQ9XoGQXMmK"
      },
      "source": [
        "Welcome to your third assignment. This exercise will test your understanding on Ensemble Methods."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "JvHYIhS-XMmL"
      },
      "outputs": [],
      "source": [
        "# Always run this cell\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# USE THE FOLLOWING RANDOM STATE FOR YOUR CODE\n",
        "RANDOM_STATE = 42"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "joKwpih2XMmM"
      },
      "source": [
        "## Download the Dataset ##\n",
        "Download the dataset using the following cell or from this [link](https://github.com/sakrifor/public/tree/master/machine_learning_course/EnsembleDataset) and put the files in the same folder as the .ipynb file. \n",
        "In this assignment you are going to work with a dataset originated from the [ImageCLEFmed: The Medical Task 2016](https://www.imageclef.org/2016/medical) and the **Compound figure detection** subtask. The goal of this subtask is to identify whether a figure is a compound figure (one image consists of more than one figure) or not. The train dataset consits of 4197 examples/figures and each figure has 4096 features which were extracted using a deep neural network. The *CLASS* column represents the class of each example where 1 is a compoung figure and 0 is not. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NJdwPr0bXMmM",
        "outputId": "70f064fa-7ee5-4985-f59d-9c94c1d1c3f2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('test_set_noclass.csv', <http.client.HTTPMessage at 0x1f985a4f4f0>)"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import urllib.request\n",
        "url_train = 'https://github.com/sakrifor/public/raw/master/machine_learning_course/EnsembleDataset/train_set.csv'\n",
        "filename_train = 'train_set.csv'\n",
        "urllib.request.urlretrieve(url_train, filename_train)\n",
        "url_test = 'https://github.com/sakrifor/public/raw/master/machine_learning_course/EnsembleDataset/test_set_noclass.csv'\n",
        "filename_test = 'test_set_noclass.csv'\n",
        "urllib.request.urlretrieve(url_test, filename_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "t0OVtYr7XMmN"
      },
      "outputs": [],
      "source": [
        "# Run this cell to load the data\n",
        "train_set = pd.read_csv(\"train_set.csv\").sample(frac=1).reset_index(drop=True)\n",
        "train_set.head()\n",
        "X = train_set.drop(columns=['CLASS'])\n",
        "y = train_set['CLASS'].values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZxOGHSmqXMmO"
      },
      "source": [
        "## 1.0 Testing different ensemble methods ##\n",
        "In this part of the assignment you are asked to create and test different ensemble methods using the train_set.csv dataset. You should use **10-fold cross validation** for your tests and report the average f-measure weighted and balanced accuracy of your models. You can use [cross_validate](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_validate.html#sklearn.model_selection.cross_validate) and select both metrics to be measured during the evaluation. Otherwise, you can use [KFold](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html#sklearn.model_selection.KFold).\n",
        "\n",
        "### !!! Use n_jobs=-1 where is posibble to use all the cores of a machine for running your tests ###"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ww_u4OlrXMmO"
      },
      "source": [
        "### 1.1 Voting ###\n",
        "Create a voting classifier which uses three **simple** estimators/classifiers. Test both soft and hard voting and choose the best one. Consider as simple estimators the following:\n",
        "\n",
        "\n",
        "*   Decision Trees\n",
        "*   Linear Models\n",
        "*   Probabilistic Models (Naive Bayes)\n",
        "*   KNN Models  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "RwvPacgkXMmP"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:   33.4s finished\n",
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:   33.6s finished\n",
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:   35.0s finished\n",
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:   26.8s finished\n"
          ]
        }
      ],
      "source": [
        "# BEGIN CODE HERE\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "from sklearn.model_selection import cross_val_score,cross_validate\n",
        "\n",
        "cls1 = DecisionTreeClassifier(random_state=42,max_depth=7) # Classifier #1 \n",
        "cls2 = KNeighborsClassifier(n_neighbors=5,n_jobs=-1)# Classifier #2 \n",
        "cls3 = LogisticRegression(random_state=42,n_jobs=-1) # Classifier #1\n",
        "soft_vcls = VotingClassifier(estimators=[(\"dtree\",cls1),(\"knn\",cls2),(\"lr\",cls3)],n_jobs=-1,voting=\"soft\") # Voting Classifier\n",
        "hard_vcls = VotingClassifier(estimators=[(\"dtree\",cls1),(\"knn\",cls2),(\"lr\",cls3)],n_jobs=-1,voting=\"hard\") # Voting Classifier\n",
        "\n",
        "svlcs_scores = cross_validate(soft_vcls, X, y, cv=10, scoring=('f1_weighted','balanced_accuracy'), n_jobs=-1)\n",
        "s_avg_fmeasure = sum(cross_val_score(soft_vcls,X,y,cv=10,scoring=\"f1_weighted\",n_jobs=-1,verbose=2))/10 # The average f-measure\n",
        "s_avg_accuracy = sum(cross_val_score(soft_vcls,X,y,cv=10,scoring=\"balanced_accuracy\",n_jobs=-1,verbose=2))/10 # The average accuracy\n",
        "# s_avg_fmeasure = cv_results['test_f1_weighted'].mean()  \n",
        "# s_avg_accuracy = cv_results['test_balanced_accuracy'].mean() \n",
        "\n",
        "hvlcs_scores = cross_validate(hard_vcls, X, y, cv=10, scoring=('f1_weighted','balanced_accuracy'), n_jobs=-1)\n",
        "h_avg_fmeasure = sum(cross_val_score(hard_vcls,X,y,cv=10,scoring=\"f1_weighted\",n_jobs=-1,verbose=2))/10 # The average f-measure\n",
        "h_avg_accuracy = sum(cross_val_score(hard_vcls,X,y,cv=10,scoring=\"balanced_accuracy\",n_jobs=-1,verbose=2))/10 # The average \n",
        "# h_avg_fmeasure = cv_results['test_f1_weighted'].mean()  \n",
        "# h_avg_accuracy = cv_results['test_balanced_accuracy'].mean() \n",
        "#END CODE HERE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "sQQvClrmXMmQ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Classifier:\n",
            "VotingClassifier(estimators=[('dtree',\n",
            "                              DecisionTreeClassifier(max_depth=7,\n",
            "                                                     random_state=42)),\n",
            "                             ('knn', KNeighborsClassifier(n_jobs=-1)),\n",
            "                             ('lr',\n",
            "                              LogisticRegression(n_jobs=-1, random_state=42))],\n",
            "                 n_jobs=-1, voting='soft')\n",
            "F1 Weighted-Score: 0.8312 & Balanced Accuracy: 0.8235\n"
          ]
        }
      ],
      "source": [
        "print(\"Classifier:\")\n",
        "print(soft_vcls)\n",
        "print(\"F1 Weighted-Score: {} & Balanced Accuracy: {}\".format(round(s_avg_fmeasure,4), round(s_avg_accuracy,4)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k-iJK9pFaDka"
      },
      "source": [
        "You should achive above 82% (Soft Voting Classifier)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "XRNkVAvEYVbn"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Classifier:\n",
            "VotingClassifier(estimators=[('dtree',\n",
            "                              DecisionTreeClassifier(max_depth=7,\n",
            "                                                     random_state=42)),\n",
            "                             ('knn', KNeighborsClassifier(n_jobs=-1)),\n",
            "                             ('lr',\n",
            "                              LogisticRegression(n_jobs=-1, random_state=42))],\n",
            "                 n_jobs=-1)\n",
            "F1 Weighted-Score: 0.8245 & Balanced Accuracy: 0.816\n"
          ]
        }
      ],
      "source": [
        "print(\"Classifier:\")\n",
        "print(hard_vcls)\n",
        "print(\"F1 Weighted-Score: {} & Balanced Accuracy: {}\".format(round(h_avg_fmeasure,4), round(h_avg_accuracy,4)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V6M0CZO6aEHi"
      },
      "source": [
        "You should achieve above 80% in both! (Hard Voting Classifier)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xVPuIxwFXMmR"
      },
      "source": [
        "### 1.2 Stacking ###\n",
        "Create a stacking classifier which uses two more complex estimators. Try different simple classifiers (like the ones mentioned before) for the combination of the initial estimators. Report your results in the following cell.\n",
        "\n",
        "Consider as complex estimators the following:\n",
        "\n",
        "*   Random Forest\n",
        "*   SVM\n",
        "*   Gradient Boosting\n",
        "*   MLP\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "HX6T1qrFXMmS"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed: 53.9min finished\n",
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed: 52.3min finished\n"
          ]
        }
      ],
      "source": [
        "# BEGIN CODE HERE\n",
        "from sklearn.ensemble import StackingClassifier,GradientBoostingClassifier,RandomForestClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "# Classifier #1 \"\n",
        "cls1 = RandomForestClassifier(random_state= 42,n_jobs=-1)\n",
        "# Classifier #2 \"\n",
        "cls2 = GradientBoostingClassifier(random_state=42)\n",
        "cls3 = SVC(random_state=42) # Classifier #3 (Optional)\n",
        "scls = StackingClassifier(estimators=[('RFC',cls1),('GBC',cls2),('SVC',cls3)],n_jobs=-1) # Stacking Classifier\n",
        "# scls = VotingClassifier(estimators=[(\"dtree\",cls1),(\"knn\",cls2)],n_jobs=-1,voting=\"soft\")\n",
        "scores =  cross_validate(scls, X, y, cv=10, scoring=('f1_weighted','balanced_accuracy'), n_jobs=-1)\n",
        "avg_fmeasure = sum(cross_val_score(scls,X,y,cv=10,scoring=\"f1_weighted\",n_jobs=-1,verbose=2))/10 # The average f-measure\n",
        "avg_accuracy = sum(cross_val_score(scls,X,y,cv=10,scoring=\"balanced_accuracy\",n_jobs=-1,verbose=2))/10 # The average accuracy\n",
        "# avg_fmeasure = cv_results['test_f1_weighted'].mean()  \n",
        "# avg_accuracy = cv_results['test_balanced_accuracy'].mean() \n",
        "#END CODE HERE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "-JLRzkQ1XMmT"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Classifier:\n",
            "StackingClassifier(estimators=[('RFC',\n",
            "                                RandomForestClassifier(n_jobs=-1,\n",
            "                                                       random_state=42)),\n",
            "                               ('GBC',\n",
            "                                GradientBoostingClassifier(random_state=42)),\n",
            "                               ('SVC', SVC(random_state=42))],\n",
            "                   n_jobs=-1)\n",
            "F1 Weighted Score: 0.8627 & Balanced Accuracy: 0.857\n"
          ]
        }
      ],
      "source": [
        "print(\"Classifier:\")\n",
        "print(scls)\n",
        "print(\"F1 Weighted Score: {} & Balanced Accuracy: {}\".format(round(avg_fmeasure,4), round(avg_accuracy,4)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zcgOx-HPvBI-"
      },
      "source": [
        "You should achieve above 85% in both\n",
        "\n",
        "Όσον αφορά το voting  βρίκα καλύτερα αποτελέσματα στο soft voting.\n",
        "\n",
        "Soft Voting F1 Weighted-Score: 0.8312 & Balanced Accuracy: 0.8235\n",
        "Hard Voting F1 Weighted-Score: 0.8245 & Balanced Accuracy: 0.816\n",
        "\n",
        "Χρησιμοποίησα decision tree , knn and logistic regression με ελάχιστες παραμέτρου (να μην έχω verfit) και έβγαλα επιθυμητά αποτελέσματα.\n",
        "\n",
        "Στο stacking μετά από εξονυχιστικά grid search και άπειρες δοκιμές βρίκα επιθυμητά αποτελέσματα με random forest , gradient boost , SVC and Stacking Classifier χωρίς παραμέτρους. Για το stacking άφησα τον meta estimator default ο οποίος είναι logistic regression.\n",
        "\n",
        "Stacking F1 Weighted Score: 0.8627 & Balanced Accuracy: 0.857\n",
        "\n",
        "Σκοπός μου ήταν να χρησιμοποιήσω estimators με διαφορετικές αδυναμίες και πρωτερίμετα με σκοπός σε καθένα από τα subsets να έχω τουλάχιστον κάποια σωστά αποτελέσματα.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O-nqW51xXMmU"
      },
      "source": [
        "## 2.0 Randomization ##"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KPG8MdFLXMmV"
      },
      "source": [
        "**2.1** You are asked to create three ensembles of decision trees where each one uses a different method for producing homogeneous ensembles. Compare them with a simple decision tree classifier and report your results in the dictionaries (dict) below using as key the given name of your classifier and as value the f1_weighted/balanced_accuracy score. The dictionaries should contain four different elements.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "PmkaP-DjXMmV"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:   13.2s finished\n",
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:   11.9s finished\n",
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:  1.8min finished\n",
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:  7.4min finished\n",
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:   12.4s finished\n",
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:   12.3s finished\n",
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:  1.9min finished\n",
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:  8.0min finished\n"
          ]
        }
      ],
      "source": [
        "# BEGIN CODE HERE\n",
        "from sklearn.ensemble import RandomForestClassifier,GradientBoostingClassifier,AdaBoostClassifier\n",
        "\n",
        "ens1 = RandomForestClassifier(max_depth=7,n_jobs=-1,random_state=42)\n",
        "ens2 = AdaBoostClassifier(random_state=42)\n",
        "ens3 = GradientBoostingClassifier(max_depth=3,random_state=42)\n",
        "tree = DecisionTreeClassifier(max_depth=7,random_state=42)\n",
        "\n",
        "f_measures = dict()\n",
        "f_score=cross_val_score(tree,X,y,cv=10,scoring=\"f1_weighted\",n_jobs=-1,verbose=2)\n",
        "f_measures[\"Simple Deciscion\"]=f_score.mean()\n",
        "f_score=cross_val_score(ens1,X,y,cv=10,scoring=\"f1_weighted\",n_jobs=-1,verbose=2)\n",
        "f_measures[\"Ensemble with Random Forest\"]=f_score.mean()\n",
        "f_score=cross_val_score(ens2,X,y,cv=10,scoring=\"f1_weighted\",n_jobs=-1,verbose=2)\n",
        "f_measures[\"Ensemble with Adaboost\"]=f_score.mean()\n",
        "f_score=cross_val_score(ens3,X,y,cv=10,scoring=\"f1_weighted\",n_jobs=-1,verbose=2)\n",
        "f_measures[\"Ensemble with Gradientboost\"]=f_score.mean()\n",
        "\n",
        "accuracies = dict()\n",
        "# Example f_measures = {'Simple Decision': 0.8551, 'Ensemble with random ...': 0.92, ...}\n",
        "acc=cross_val_score(tree,X,y,cv=10,scoring=\"balanced_accuracy\",n_jobs=-1,verbose=2)\n",
        "accuracies[\"Simple Deciscion\"]=acc.mean()\n",
        "acc=cross_val_score(ens1,X,y,cv=10,scoring=\"balanced_accuracy\",n_jobs=-1,verbose=2)\n",
        "accuracies[\"Ensemble with Random Forest\"]=acc.mean()\n",
        "acc=cross_val_score(ens2,X,y,cv=10,scoring=\"balanced_accuracy\",n_jobs=-1,verbose=2)\n",
        "accuracies[\"Ensemble with Adaboost\"]=acc.mean()\n",
        "acc=cross_val_score(ens3,X,y,cv=10,scoring=\"balanced_accuracy\",n_jobs=-1,verbose=2)\n",
        "accuracies[\"Ensemble with Gradientboost\"]=acc.mean()\n",
        "#END CODE HERE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "IUqhDUuCXMmW"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RandomForestClassifier(max_depth=7, n_jobs=-1, random_state=42)\n",
            "AdaBoostClassifier(random_state=42)\n",
            "GradientBoostingClassifier(random_state=42)\n",
            "DecisionTreeClassifier(max_depth=7, random_state=42)\n",
            "Classifier:Simple Deciscion -  F1 Weighted:0.7165\n",
            "Classifier:Ensemble with Random Forest -  F1 Weighted:0.7582\n",
            "Classifier:Ensemble with Adaboost -  F1 Weighted:0.7825\n",
            "Classifier:Ensemble with Gradientboost -  F1 Weighted:0.8175\n",
            "Classifier:Simple Deciscion -  BalancedAccuracy:0.704\n",
            "Classifier:Ensemble with Random Forest -  BalancedAccuracy:0.7371\n",
            "Classifier:Ensemble with Adaboost -  BalancedAccuracy:0.7739\n",
            "Classifier:Ensemble with Gradientboost -  BalancedAccuracy:0.8064\n"
          ]
        }
      ],
      "source": [
        "print(ens1)\n",
        "print(ens2)\n",
        "print(ens3)\n",
        "print(tree)\n",
        "for name,score in f_measures.items():\n",
        "    print(\"Classifier:{} -  F1 Weighted:{}\".format(name,round(score,4)))\n",
        "for name,score in accuracies.items():\n",
        "    print(\"Classifier:{} -  BalancedAccuracy:{}\".format(name,round(score,4)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UqdXTE_2XMmX"
      },
      "source": [
        "**2.2** Describe your classifiers and your results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rU9POFftXMmX"
      },
      "source": [
        "YOUR ANSWER HERE\n",
        "Deciscion Trees:Τα δέντρα απόφασης χρησιμοποιούν πολλαπλούς αλγόριθμους για να αποφασίσουν να χωρίσουν ανάλογα με το είδος του δέντρου , χρησιμοποιούν εντροπία ή gini και information gain για να διαχωρίσουν έναν κόμβο σε δύο ή περισσότερους υπο-κόμβους. Υπολογίζεται για το κάθε χαρακτηριστικό ξεχωριστά και έτσι αποφασίζει ποιός θα είναι ο πατέρας και ποιό το παιδί. Αυτή η διαδικασία συνεχίζεται μέχρι να κατασκευαστεί το δέντρο.\n",
        " \n",
        "Random Forest: Ο random forest δημιουργεί πολλά δέντρα αποφάσεων για να πάρει μια πιο ακριβή και σταθερή πρόβλεψη.Επιλέγει τυχαία χαρακτηριστικά για το κάθε δέντρο που χρησιμοποιεί από bootstrap δεδομένα. O random forest έχει σχεδόν τις ίδιες υπερπαραμέτρους με ένα δέντρο αποφάσεων ή έναν bagging classifier.Το τυχαίο δάσος προσθέτει επιπλέον τυχαιότητα στο μοντέλο, ενώ καλλιεργεί τα δέντρα. Στο τέλος η απόφαση γίνεται με βάση τη προβλέψανε τα δέντρα του δάσους.Στο τέλος χρησιμοποιά τα out of the bag στιγμιότυπα για να αξιολογήσει το μοντέλο.\n",
        " \n",
        "Gradient Boost: Το Gradient Boosting είναι ένας επαναληπτικός functional gradient algorithm, δηλαδή ένας αλγόριθμος που ελαχιστοποιεί μια συνάρτηση απώλειας επιλέγοντας επαναληπτικά μια συνάρτηση που δείχνει προς την negative gradient.Παράγει δέντρα κάθε φορά προσπαθώντας να διορθώσει τα λαθοι του προηγούμενου δέντρου μειώνοντας το σφάλμα του στις προβλέψεις και δίνοντας βαροι στα δέντρα ανάλογα με τα πιό σωστά.\n",
        " \n",
        "Adaboost:Ο ταξινομητής αυτός , δημιουργεί stumps (δέντρα με 1 γονιό , 2 παιδιά) για κάθε χαρακτηριστικό και σύμφωνα με τα λάθοι που έκανε , δίνει βάρος στα δεδομένα που προέβλεψε λάθος έτσι ώστε στην επόμενη δημιουργία να προσπαθήσει να τα διορθώσει. Είναι επίσης ένας επαναληπτικός αλγόριθμος ο οποίος προσπαθεί να διορθώσει τα λάθη του.\n",
        " \n",
        " \n",
        "Εφάρμοσα τους ποιό πάνω ταξινομητές που χρησιμοποιούν δέντρα για την δημιουργία τους. Οι μόνοι περιορισμοί που χρησιμοποίησα είναι τα max depth για να αποφύγω overfitting . Δεν χρειάστηκαν άλλοι περιορισμοί καθώς βγάζω ικανοποιητικά αποτελέσματα. Ο καλύτερος στην περίπτωση μου είναι ο gradient boost. Είναι λογικό γιατί παρά την αυξημένη πολυπλοκότητα του , προσπαθεί να διορθώσει τα λάθη του και διορθώνει κάποιες από τις αδυναμίες των άλλων μοντέλων.\n",
        " \n",
        "Γενικά προσπάθησα να χρησιμοποιήσω ταξινομητές με διαφορετικές συμπεριφορές για να διορθώνει ο ένας τις αδυναμίες του άλλου και βλέπω ότι τα καλύτερα αποτελέσματα τα έχω με τον gradient boost\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lkJeuV1FXMmX"
      },
      "source": [
        "**2.3** Increasing the number of estimators in a bagging classifier can drastically increase the training time of a classifier. Is there any solution to this problem? Can the same solution be applied to boosting classifiers?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ApNEPcWEXMmY"
      },
      "source": [
        "YOUR ANSWER HERE\n",
        "Στο bagging εκπαιδεύονται μοντέλα ανεξάρτητα μεταξύ τους άρα η διαδικασία μπορεί να γίνει και παράλληλα(παραλληλισμός), έτσι μειώνεται αρκετά ο χρόνος εκπαίδευσης. Δυστυχώς αυτό στα άλλα boosting μοντέλα δεν μπορούμε να το κάνουμε γιατί κάθε δέντρο που δημιουργείται είναι σε σχέση με το προηγούμενο. n_jobs = -1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XgvsCbUGXMmY"
      },
      "source": [
        "## 3.0 Creating the best classifier ##"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q6daX2mRXMmZ"
      },
      "source": [
        "**3.1** In this part of the assignment you are asked to train the best possible ensemble! Describe the process you followed to achieve this result. How did you choose your classifier and your parameters and why. Report the f-measure (weighted) & balanced accuracy (10-fold cross validation) of your final classifier and results of classifiers you tried in the cell following the code. Can you achieve a balanced accuracy over 83-84%?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "00xAQ0HfXMmZ"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:  5.2min finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'fit_time': array([114.92062807, 102.15733719, 117.88398814, 120.37117958,\n",
            "        99.61327195, 121.2740984 , 120.09749103, 116.92477536,\n",
            "        88.10658216,  71.27705884]), 'score_time': array([0.17504573, 0.17805672, 0.18105221, 0.17394137, 0.17388153,\n",
            "       0.17703176, 0.15895104, 0.17488432, 0.14762664, 0.07384515]), 'test_balanced_accuracy': array([0.82626196, 0.84448059, 0.86644357, 0.85401699, 0.85777305,\n",
            "       0.86210245, 0.874694  , 0.84107571, 0.8389492 , 0.84974623]), 'test_f1_weighted': array([0.83302802, 0.85378895, 0.87131403, 0.86137086, 0.86397067,\n",
            "       0.8645585 , 0.87833458, 0.84898478, 0.84063263, 0.85640104])}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:  5.1min finished\n",
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:  5.5min finished\n"
          ]
        }
      ],
      "source": [
        "# BEGIN CODE HERE\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "from sklearn.model_selection import cross_val_score,cross_validate\n",
        "\n",
        "# cls1 = RandomForestClassifier(random_state= 42,n_jobs=-1)\n",
        "# Classifier #2 \"\n",
        "# cls2 = GradientBoostingClassifier(random_state=42)\n",
        "# cls3 = SVC(random_state=42) # Classifier #3 (Optional)\n",
        "# scls = StackingClassifier(estimators=[('RFC',cls1),('GBC',cls2),('SVC',cls3)],n_jobs=-1) # Stacking Classifier\n",
        "# # scls = VotingClassifier(estimators=[(\"dtree\",cls1),(\"knn\",cls2)],n_jobs=-1,voting=\"soft\")\n",
        "# scores =  cross_validate(scls, X, y, cv=10, scoring=('f1_weighted','balanced_accuracy'), n_jobs=-1)\n",
        "\n",
        "\n",
        "mlp1 = MLPClassifier(random_state=42, activation='relu', hidden_layer_sizes=(50, 50),\n",
        "                     learning_rate='constant')\n",
        "mlp2 = MLPClassifier(random_state=42, activation='relu', hidden_layer_sizes=(50, 50, 50),\n",
        "                     learning_rate='constant')\n",
        "mlp3 = MLPClassifier(random_state=42, activation='relu', hidden_layer_sizes=(50, 50, 50, 50),\n",
        "                     learning_rate='constant')\n",
        "# Χρησιμοποιήθηκε voting και όχι stacking λόγω χρόνου...\n",
        "cls = VotingClassifier(estimators=[('mlp1', mlp1), ('mlp2', mlp2), ('mlp3', mlp3) ],\n",
        "                            voting='hard')\n",
        "best_cls = Pipeline([\n",
        "    ('scale', StandardScaler()),\n",
        "    ('voting', cls)])\n",
        "\n",
        "# Cross validation\n",
        "cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
        "cv_results = cross_validate(best_cls, X, y, cv=10, n_jobs=-1, verbose=1, scoring=['balanced_accuracy', 'f1_weighted'])\n",
        "\n",
        "print(cv_results)\n",
        "\n",
        "best_fmeasure =sum(cross_val_score(best_cls,X,y,cv=10,scoring=\"f1_weighted\",n_jobs=-1,verbose=2))/10\n",
        "best_accuracy = sum(cross_val_score(best_cls,X,y,cv=10,scoring=\"balanced_accuracy\",n_jobs=-1,verbose=2))/10\n",
        "\n",
        "# best_fmeasure = cv_results['test_f1_weighted'].mean()  \n",
        "# best_accuracy = cv_results['test_balanced_accuracy'].mean() \n",
        "\n",
        "#END CODE HERE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FbLB09agXMma",
        "outputId": "be9abfc9-aa0d-4e22-9f07-46926fd1d987"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Classifier:\n",
            "F1 Weighted-Score:0.8572384066630363 & Balanced Accuracy:0.8515543746345948\n"
          ]
        }
      ],
      "source": [
        "print(\"Classifier:\")\n",
        "#print(best_cls)\n",
        "print(\"F1 Weighted-Score:{} & Balanced Accuracy:{}\".format(best_fmeasure, best_accuracy))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vnos1uqzXMma"
      },
      "source": [
        "**3.2** Describe the process you followed to achieve this result. How did you choose your classifier and your parameters and why. Report the f-measure & accuracy (10-fold cross validation) of your final classifier and results of classifiers you tried in the cell following the code."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o5dAfbTfXMmb"
      },
      "source": [
        "Αρχικά προσπάθησα να ξανακάνω gridsearch για τα καλύτερα hyper parameters των πιο καλών μοντέλων μου βρήκα πάνω , δηλαδή stacking με andom forest , gradientboost , SVC and StackingClassifier αλλά δεν ήμουν ικανοποιημένος με τα αποτελέσματά μου κυρίως γιατί ήθελε πολλή ώρα να τρέξει. Έτσι πειραματίστηκα και με άλλους αλγόριθμους όπως τα νευρωνικά και με την βοήθεια του internet δοκίμασα να κάνω stacking με νευρωνικά διαφορετικών hidden layers . https://stackoverflow.com/questions/34723489/how-do-multiple-hidden-layers-in-a-neural-network-improve-its-ability-to-learn . Γιαυτό για να μην βάλω απλά πολλά layers και να έχω verfiting έκανα stacking 3 διαφορετικών για να έχουν γνώμη όλοι στο τελικό αποτέλεσμα. Τελικά έχω καλύτερα αποτελέσματα με αυτό τον τρόπο.Piplene και scaler δεν χρειαζόταν αλλά επειδή έχω σκοπό να το χρησιμοποιήσω και στην 3.3 το έβαλα ος παράδειγμά. Παρόλο που δεν χρειάζεται στην συγκεκριμένη εργασία να κάνουμε preprocessing όταν πρέπει να κάνουμε deploy το μοντέλο μας για νομιμοποιηθεί , θα πρέπει να επεξεργαστούμε τα δεδομένα."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fQEFCmbcXMmb"
      },
      "source": [
        "**3.3** Create a classifier that is going to be used in production - in a live system. Use the *test_set_noclass.csv* to make predictions. Store the predictions in a list.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XQPgm_ubXMmc"
      },
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "'ellipsis' object has no attribute 'predict'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[1;32mc:\\Users\\user1\\Desktop\\mixaniki mathisi\\Ensembles\\EnsembleMethods.ipynb Cell 36'\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/user1/Desktop/mixaniki%20mathisi/Ensembles/EnsembleMethods.ipynb#ch0000035?line=2'>3</a>\u001b[0m \u001b[39m#END CODE HERE\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/user1/Desktop/mixaniki%20mathisi/Ensembles/EnsembleMethods.ipynb#ch0000035?line=3'>4</a>\u001b[0m test_set \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(\u001b[39m\"\u001b[39m\u001b[39mtest_set_noclass.csv\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/user1/Desktop/mixaniki%20mathisi/Ensembles/EnsembleMethods.ipynb#ch0000035?line=4'>5</a>\u001b[0m predictions \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49mpredict(test_set)\n",
            "\u001b[1;31mAttributeError\u001b[0m: 'ellipsis' object has no attribute 'predict'"
          ]
        }
      ],
      "source": [
        "# BEGIN CODE HERE\n",
        "cls = best_cls\n",
        "\n",
        "cls.fit(X, y)\n",
        "# predictions_list = list(cls.predict(test_set))\n",
        "#END CODE HERE\n",
        "test_set = pd.read_csv(\"test_set_noclass.csv\")\n",
        "predictions = cls.predict(test_set)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lnAp-d2DXMmf"
      },
      "source": [
        "LEAVE HERE ANY COMMENTS ABOUT YOUR CLASSIFIER\n",
        "\n",
        "Μπορούσα να χρησημοποιήσω τους πύνακες των scores για να βρώ mean() αλλά άργισα να το δώ και τα έτρεξα με έξτρα εντολές, είναι λάθος αλλά δεν προλαμβένω να τρέξω ξανά όλο το πρόγραμμα. Θα γλύτωνα και αρκετό χρώνο."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Neagvu0TXMmg"
      },
      "source": [
        "#### This following cell will not be executed. The test_set.csv with the classes will be made available after the deadline and this cell is for testing purposes!!! Do not modify it! ###"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k7K7iI7BXMmg"
      },
      "outputs": [],
      "source": [
        "if False:\n",
        "  from sklearn.metrics import f1_score, balanced_accuracy_score\n",
        "  final_test_set = pd.read_csv('test_set.csv')\n",
        "  ground_truth = final_test_set['CLASS']\n",
        "  print(\"Balanced Accuracy: {}\".format(balanced_accuracy_score(predictions, ground_truth)))\n",
        "  print(\"F1 Weighted-Score: {}\".format(f1_score(predictions, ground_truth, average='weighted')))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YJH-9KdOzW7z"
      },
      "source": [
        "Both should aim above 85%!"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "EnsembleMethods.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "11938c6bc6919ae2720b4d5011047913343b08a43b18698fd82dedb0d4417594"
    },
    "kernelspec": {
      "display_name": "Python 3.9.5 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
